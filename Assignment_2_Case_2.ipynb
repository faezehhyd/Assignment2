{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8m24JAOcoWCslm4Q5Vvbc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faezehhyd/Assignment_2/blob/main/Assignment_2_Case_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMHbhvEHpkSy",
        "outputId": "fb210671-5af6-4231-8a02-a9b0c1408080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-22.2.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-22.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from faker import Faker\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PANv4abxpsW1",
        "outputId": "c3acfecd-8add-44a5-ff0a-c0db7768878e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Generate a word frequency table from the given text.\n",
        "This table contains the frequency of each word in the text, excluding stop words.\n",
        "\"\"\"\n",
        "def generate_word_frequency_table(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    word_frequency_table = FreqDist(filtered_words)\n",
        "    print(\"generating word frequency table:\")\n",
        "    return word_frequency_table\n",
        "\n",
        "\n",
        "# Tokenize the given text into sentences.\n",
        "def tokenize_sentences(text):\n",
        "    print(\"tokenizing sentences:\")\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "# Score each sentence based on term frequency.\n",
        "def score_sentences(sentences, word_frequency_table):\n",
        "    print(\"scoring sentences:\")\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word, freq in word_frequency_table.items():\n",
        "            if word in sentence.lower():\n",
        "                if sentence[:10] in sentence_scores:\n",
        "                    sentence_scores[sentence[:10]] += freq\n",
        "                else:\n",
        "                    sentence_scores[sentence[:10]] = freq\n",
        "\n",
        "    # Normalize scores by dividing by the number of words in each sentence\n",
        "    for sentence, score in sentence_scores.items():\n",
        "        sentence_scores[sentence] = score / len(sentence.split())\n",
        "\n",
        "    return sentence_scores\n",
        "\n",
        "\n",
        "# Find the threshold by calculating the average score of sentences.\n",
        "def find_threshold(sentence_scores):\n",
        "    print(\"finding threshold:\")\n",
        "    # Calculate the average score as the threshold\n",
        "    return sum(sentence_scores.values()) / len(sentence_scores)\n",
        "\n",
        "\n",
        "# Find the threshold by calculating the average score of sentences.\n",
        "def generate_summary(sentences, sentence_scores, threshold):\n",
        "    summary = \"\"\n",
        "    for sentence in sentences:\n",
        "        if sentence[:10] in sentence_scores and sentence_scores[sentence[:10]] >= threshold:\n",
        "            summary += \" \" + sentence\n",
        "    print(\"generating summary:\")\n",
        "    print(\"\\n\")\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Generate a fake document with the specified number of paragraphs.\n",
        "def generate_fake_document(paragraphs):\n",
        "    print(\"generating fake documents:\")\n",
        "    fake = Faker()\n",
        "    fake_document = \"\"\n",
        "    for _ in range(paragraphs):\n",
        "        fake_document += fake.paragraph() + \"\\n\"\n",
        "    return fake_document\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the number of fake documents and length in terms of paragraphs\n",
        "    num_fake_documents = 5\n",
        "    paragraphs_per_document = 1000\n",
        "\n",
        "    print(\"number of documents : \",num_fake_documents)\n",
        "    print(\"paragraphs per documents :\",paragraphs_per_document )\n",
        "    print(\"\\n\")\n",
        "\n",
        "    for i in range(num_fake_documents):\n",
        "        fake_document = generate_fake_document(paragraphs_per_document)\n",
        "\n",
        "        word_frequency_table = generate_word_frequency_table(fake_document)\n",
        "        sentences = tokenize_sentences(fake_document)\n",
        "        sentence_scores = score_sentences(sentences, word_frequency_table)\n",
        "        threshold = find_threshold(sentence_scores)\n",
        "        summary = generate_summary(sentences, sentence_scores, threshold)\n",
        "\n",
        "        # Save documents and summary to files\n",
        "        with open(f\"fake_document_{i+1}.txt\", \"w\", encoding=\"utf-8\") as doc_file:\n",
        "            doc_file.write(fake_document)\n",
        "\n",
        "        with open(f\"summary_{i+1}.txt\", \"w\", encoding=\"utf-8\") as summary_file:\n",
        "            summary_file.write(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLmHGLwSp7Ej",
        "outputId": "6b0e807f-19a7-4e58-c5d8-b0c3f39645ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of documents :  5\n",
            "paragraphs per documents : 1000\n",
            "\n",
            "\n",
            "generating fake documents:\n",
            "generating word frequency table:\n",
            "tokenizing sentences:\n",
            "scoring sentences:\n",
            "finding threshold:\n",
            "generating summary:\n",
            "\n",
            "\n",
            "generating fake documents:\n",
            "generating word frequency table:\n",
            "tokenizing sentences:\n",
            "scoring sentences:\n",
            "finding threshold:\n",
            "generating summary:\n",
            "\n",
            "\n",
            "generating fake documents:\n",
            "generating word frequency table:\n",
            "tokenizing sentences:\n",
            "scoring sentences:\n",
            "finding threshold:\n",
            "generating summary:\n",
            "\n",
            "\n",
            "generating fake documents:\n",
            "generating word frequency table:\n",
            "tokenizing sentences:\n",
            "scoring sentences:\n",
            "finding threshold:\n",
            "generating summary:\n",
            "\n",
            "\n",
            "generating fake documents:\n",
            "generating word frequency table:\n",
            "tokenizing sentences:\n",
            "scoring sentences:\n",
            "finding threshold:\n",
            "generating summary:\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}