{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBmoUvPu6ScrinPEkXlUYu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faezehhyd/NLP_Assignment_2/blob/main/Assignment_2_Case_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMHbhvEHpkSy",
        "outputId": "3f75ba60-1f1f-417e-f641-33ea56170dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-22.2.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-22.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from faker import Faker\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PANv4abxpsW1",
        "outputId": "2932cf71-5d54-4064-d07e-805e084de47a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Generate a word frequency table from the given text.\n",
        "This table contains the frequency of each word in the text, excluding stop words.\n",
        "\"\"\"\n",
        "def generate_word_frequency_table(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    word_frequency_table = FreqDist(filtered_words)\n",
        "    return word_frequency_table\n",
        "\n",
        "\n",
        "# Tokenize the given text into sentences.\n",
        "def tokenize_sentences(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "# Score each sentence based on term frequency.\n",
        "def score_sentences(sentences, word_frequency_table):\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word, freq in word_frequency_table.items():\n",
        "            if word in sentence.lower():\n",
        "                if sentence[:10] in sentence_scores:\n",
        "                    sentence_scores[sentence[:10]] += freq\n",
        "                else:\n",
        "                    sentence_scores[sentence[:10]] = freq\n",
        "\n",
        "    # Normalize scores by dividing by the number of words in each sentence\n",
        "    for sentence, score in sentence_scores.items():\n",
        "        sentence_scores[sentence] = score / len(sentence.split())\n",
        "\n",
        "    return sentence_scores\n",
        "\n",
        "\n",
        "# Find the threshold by calculating the average score of sentences.\n",
        "def find_threshold(sentence_scores):\n",
        "    # Calculate the average score as the threshold\n",
        "    return sum(sentence_scores.values()) / len(sentence_scores)\n",
        "\n",
        "\n",
        "# Generate a summary by selecting sentences with scores above the threshold.\n",
        "def generate_summary(sentences, sentence_scores, threshold, max_tokens):\n",
        "    summary = \"\"\n",
        "    total_tokens = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:10] in sentence_scores and sentence_scores[sentence[:10]] >= threshold:\n",
        "            if total_tokens + len(sentence.split()) <= max_tokens:\n",
        "                summary += \" \" + sentence\n",
        "                total_tokens += len(sentence.split())\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Generate a fake document with the specified number of paragraphs.\n",
        "def generate_fake_document(paragraphs):\n",
        "    fake = Faker()\n",
        "    fake_document = \"\"\n",
        "    total_tokens = 0\n",
        "\n",
        "    for _ in range(paragraphs):\n",
        "        paragraph = fake.paragraph() + \"\\n\"\n",
        "        fake_document += paragraph\n",
        "        total_tokens += len(word_tokenize(paragraph))\n",
        "\n",
        "    return fake_document, total_tokens\n",
        "\n",
        "def collate_summaries(document_summaries, steps):\n",
        "    # Collate document summaries based on the specified number of steps.\n",
        "    collated_summaries = []\n",
        "    for i in range(0, len(document_summaries), steps):\n",
        "        step_summaries = document_summaries[i:i+steps]\n",
        "        collated_summary = \" \".join(step_summaries)\n",
        "        collated_summaries.append(collated_summary)\n",
        "    return collated_summaries\n",
        "\n",
        "\n",
        "def hierarchical_summarization(num_fake_documents, paragraphs_per_document, steps, max_tokens):\n",
        "    total_tokens_all_documents = 0\n",
        "\n",
        "    # Create a file for the summary of summaries\n",
        "    summary_of_summaries_file = open(\"summary_of_summaries.txt\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "    for i in range(num_fake_documents):\n",
        "        fake_document, total_tokens = generate_fake_document(paragraphs_per_document)\n",
        "        total_tokens_all_documents += total_tokens\n",
        "        print(\"generating fake documents number\", i + 1)\n",
        "        print(\"total tokens in document:\",total_tokens)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    print(\"\\nTotal tokens for all documents:\", total_tokens_all_documents)\n",
        "\n",
        "    # Check if the total tokens for all documents exceed the max_tokens (context window size)\n",
        "    if total_tokens_all_documents > max_tokens:\n",
        "        print(\"Exceeds the context window size. Summarizing all documents.\")\n",
        "        print(\"\\n\")\n",
        "    else:\n",
        "        print(\"Within the context window size. No summarization needed.\")\n",
        "        return\n",
        "\n",
        "    # Summarize all documents\n",
        "    for i in range(num_fake_documents):\n",
        "        fake_document, _ = generate_fake_document(paragraphs_per_document)\n",
        "        print(\"Summarizing Document\", i+1)\n",
        "        word_frequency_table = generate_word_frequency_table(fake_document)\n",
        "        print(\"\\tgenerating word frequency table\")\n",
        "        sentences = tokenize_sentences(fake_document)\n",
        "        print(\"\\ttokenizing sentences\")\n",
        "        sentence_scores = score_sentences(sentences, word_frequency_table)\n",
        "        print(\"\\tscoring sentences\")\n",
        "        threshold = find_threshold(sentence_scores)\n",
        "        print(\"\\tfinding threshold\")\n",
        "        document_summaries = []\n",
        "        print(\"\\tgenerate summary\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        for _ in range(steps):\n",
        "            summary = generate_summary(sentences, sentence_scores, threshold, max_tokens)\n",
        "            document_summaries.append(summary)\n",
        "\n",
        "            # Collate summaries\n",
        "            collated_summaries = collate_summaries(document_summaries, steps)\n",
        "\n",
        "            # Recursive summarization\n",
        "            sentences = tokenize_sentences(\" \".join(collated_summaries))\n",
        "            word_frequency_table = generate_word_frequency_table(\" \".join(collated_summaries))\n",
        "            sentence_scores = score_sentences(sentences, word_frequency_table)\n",
        "\n",
        "        # Save documents and summaries to files\n",
        "        with open(f\"fake_document_{i + 1}.txt\", \"w\", encoding=\"utf-8\") as doc_file:\n",
        "            doc_file.write(fake_document)\n",
        "\n",
        "        # Save the last collated summary to the summary file\n",
        "        with open(f\"summary_{i + 1}.txt\", \"w\", encoding=\"utf-8\") as summary_file:\n",
        "            summary_file.write(collated_summaries[-1])\n",
        "\n",
        "        # Append the last collated summary to the summary of summaries file\n",
        "        summary_of_summaries_file.write(collated_summaries[-1] + \"\\n\")\n",
        "\n",
        "    summary_of_summaries_file.close()\n",
        "    print(\"Creating summary of summaries\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the number of fake documents, length in terms of paragraphs, number of steps, and max tokens\n",
        "    num_fake_documents = 5\n",
        "    paragraphs_per_document = 1000\n",
        "    steps = 3\n",
        "    max_tokens = 4096  # maximum allowed tokens in the context window\n",
        "\n",
        "    print(\"number of documents:\", num_fake_documents)\n",
        "    print(\"paragraphs per document:\", paragraphs_per_document)\n",
        "    print(\"number of steps:\", steps)\n",
        "    print(\"max tokens(maximum allowed tokens in the context window):\", max_tokens)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    hierarchical_summarization(num_fake_documents, paragraphs_per_document, steps, max_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLmHGLwSp7Ej",
        "outputId": "a7b6ec6d-e0aa-415f-d0b8-6434c2e42a91"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of documents: 5\n",
            "paragraphs per document: 1000\n",
            "number of steps: 3\n",
            "max tokens(maximum allowed tokens in the context window): 4096\n",
            "\n",
            "\n",
            "generating fake documents number 1\n",
            "total tokens in document: 16417\n",
            "\n",
            "\n",
            "generating fake documents number 2\n",
            "total tokens in document: 16250\n",
            "\n",
            "\n",
            "generating fake documents number 3\n",
            "total tokens in document: 16153\n",
            "\n",
            "\n",
            "generating fake documents number 4\n",
            "total tokens in document: 15986\n",
            "\n",
            "\n",
            "generating fake documents number 5\n",
            "total tokens in document: 16229\n",
            "\n",
            "\n",
            "\n",
            "Total tokens for all documents: 81035\n",
            "Exceeds the context window size. Summarizing all documents.\n",
            "\n",
            "\n",
            "Summarizing Document 1\n",
            "\tgenerating word frequency table\n",
            "\ttokenizing sentences\n",
            "\tscoring sentences\n",
            "\tfinding threshold\n",
            "\tgenerate summary\n",
            "\n",
            "\n",
            "Summarizing Document 2\n",
            "\tgenerating word frequency table\n",
            "\ttokenizing sentences\n",
            "\tscoring sentences\n",
            "\tfinding threshold\n",
            "\tgenerate summary\n",
            "\n",
            "\n",
            "Summarizing Document 3\n",
            "\tgenerating word frequency table\n",
            "\ttokenizing sentences\n",
            "\tscoring sentences\n",
            "\tfinding threshold\n",
            "\tgenerate summary\n",
            "\n",
            "\n",
            "Summarizing Document 4\n",
            "\tgenerating word frequency table\n",
            "\ttokenizing sentences\n",
            "\tscoring sentences\n",
            "\tfinding threshold\n",
            "\tgenerate summary\n",
            "\n",
            "\n",
            "Summarizing Document 5\n",
            "\tgenerating word frequency table\n",
            "\ttokenizing sentences\n",
            "\tscoring sentences\n",
            "\tfinding threshold\n",
            "\tgenerate summary\n",
            "\n",
            "\n",
            "Creating summary of summaries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5XF2kQBlmRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}